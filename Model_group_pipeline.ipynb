{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# IT24100967_Logistic_regression"
      ],
      "metadata": {
        "id": "o1OL5DbSAVhE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNYj00uAARuA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    ConfusionMatrixDisplay,\n",
        "    RocCurveDisplay,\n",
        "    roc_auc_score\n",
        ")\n",
        "\n",
        "# 1. Load Data\n",
        "file_path = \"preprocessed_stress_level_dataset.csv\"\n",
        "\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "\n",
        "# 2. Define Features (X) and Target (y)\n",
        "target_column = 'stress_level'\n",
        "\n",
        "\n",
        "X = df.drop(target_column, axis=1)\n",
        "y = df[target_column]\n",
        "class_names = [f'Class {c}' for c in sorted(y.unique())]\n",
        "\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Features: {len(X.columns)}, Target: {target_column}\")\n",
        "print(f\"Target classes: {sorted(y.unique())}\\n\")\n",
        "\n",
        "# 3. Split Data\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"Training set: {X_train.shape}, Test set: {X_test.shape}\\n\")\n",
        "\n",
        "# 4. Helper function to evaluate models\n",
        "# This avoids repeating the same print/plot code for every model\n",
        "def evaluate_model(model, X_test, y_test, model_name):\n",
        "    print(f\"--- Evaluation for: {model_name} ---\")\n",
        "\n",
        "    # Get predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_prob = model.predict_proba(X_test)\n",
        "\n",
        "    # 1. Accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"1. Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    # 2. Classification Report (Precision, Recall, F1-Score)\n",
        "    print(\"\\n2. Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=class_names))\n",
        "\n",
        "    # 3. AUC Score\n",
        "    auc = roc_auc_score(y_test, y_prob, multi_class='ovr')\n",
        "    print(f\"3. AUC Score (One-vs-Rest): {auc:.4f}\")\n",
        "\n",
        "\n",
        "    # 4. Confusion Matrix\n",
        "    print(\"\\n4. Confusion Matrix:\")\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    # Plot the confusion matrix\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
        "    disp.plot(cmap=plt.cm.Blues)\n",
        "    plt.title(f\"Confusion Matrix - {model_name}\")\n",
        "    plt.savefig(f\"cm_{model_name.replace(' ', '_').lower()}.png\") # Save the plot\n",
        "    plt.show() # Display the plot\n",
        "\n",
        "    print(\"--------------------------------------------------\\n\")\n",
        "    return accuracy, y_prob # Return accuracy for comparison\n",
        "\n",
        "# To store results for final comparison\n",
        "model_accuracies = {}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Variation 1: Baseline Model (Default Parameters) ---\n",
        "# Using 'auto' for multi_class, 'l2' penalty, C=1.0, and 'lbfgs' solver are common defaults.\n",
        "model_1 = LogisticRegression(random_state=42, max_iter=2000, multi_class='auto')\n",
        "model_1.fit(X_train, y_train)\n",
        "acc1, prob1 = evaluate_model(model_1, X_test, y_test, \"Variation 1: Baseline (Defaults)\")\n",
        "model_accuracies[\"Baseline\"] = acc1"
      ],
      "metadata": {
        "id": "K9hGWqXSAhEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Variation 2: L1 (Lasso) Regularization ---\n",
        "# L1 can be used for feature selection. Must use a compatible solver like 'saga'.\n",
        "# We'll use C=1.0 (standard regularization strength).\n",
        "model_2 = LogisticRegression(\n",
        "    random_state=42,\n",
        "    max_iter=2000,\n",
        "    multi_class='auto',\n",
        "    penalty='l1',\n",
        "    C=1.0,\n",
        "    solver='saga'\n",
        ")\n",
        "model_2.fit(X_train, y_train)\n",
        "acc2, prob2 = evaluate_model(model_2, X_test, y_test, \"Variation 2: L1 (Lasso), C=1.0\")\n",
        "model_accuracies[\"L1 (Lasso), C=1.0\"] = acc2"
      ],
      "metadata": {
        "id": "4dv9pKRyAi7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Variation 3: L2 (Ridge) Regularization (Stronger) ---\n",
        "# L2 is the default penalty, but we'll use a smaller C (C=0.1).\n",
        "# A smaller C value means *stronger* regularization.\n",
        "model_3 = LogisticRegression(\n",
        "    random_state=42,\n",
        "    max_iter=2000,\n",
        "    multi_class='auto',\n",
        "    penalty='l2',\n",
        "    C=0.1,\n",
        "    solver='saga' # Using 'saga' for consistency\n",
        ")\n",
        "model_3.fit(X_train, y_train)\n",
        "acc3, prob3 = evaluate_model(model_3, X_test, y_test, \"Variation 3: L2 (Ridge), C=0.1\")\n",
        "model_accuracies[\"L2 (Ridge), C=0.1\"] = acc3"
      ],
      "metadata": {
        "id": "ztW4zxuSAk6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Variation 4: GridSearchCV Tuned Model ---\n",
        "print(\"\\n--- Starting Variation 4: GridSearchCV Tuning ---\")\n",
        "# Define the parameter grid to search\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 50],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['saga'] # 'saga' handles both l1 and l2\n",
        "}\n",
        "# Base model for grid search\n",
        "lr_base = LogisticRegression(random_state=42, max_iter=2000, multi_class='auto')\n",
        "\n",
        "# Set up GridSearch with 5-fold cross-validation\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=lr_base,\n",
        "    param_grid=param_grid,\n",
        "    cv=5, # 5-fold cross-validation\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1, # Use all available cores\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Train the grid search\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "print(f\"\\nGridSearchCV Best Parameters: {grid_search.best_params_}\")\n",
        "print(f\"GridSearchCV Best CV Accuracy: {grid_search.best_score_:.4f}\\n\")\n",
        "\n",
        "# Evaluate the best model found by GridSearchCV\n",
        "acc4, prob4 = evaluate_model(best_model, X_test, y_test, \"Variation 4: GridSearchCV-Tuned\")\n",
        "model_accuracies[\"GridSearchCV-Tuned\"] = acc4"
      ],
      "metadata": {
        "id": "D5_LCQkvAnC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 6. Final Comparison ---\n",
        "\n",
        "# Print a summary of accuracies\n",
        "print(\"\\n--- Final Model Accuracy Comparison ---\")\n",
        "print(pd.Series(model_accuracies).sort_values(ascending=False))\n",
        "\n"
      ],
      "metadata": {
        "id": "q6TRIeKNApIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IT24104387_KNN"
      ],
      "metadata": {
        "id": "SOvgkOHrArrP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    ConfusionMatrixDisplay\n",
        ")\n",
        "\n",
        "# 1. Load Data\n",
        "file_name = 'preprocessed_stress_level_dataset.csv'\n",
        "df = pd.read_csv(file_name)\n",
        "\n",
        "# 2. Define Features (X) and Target (y)\n",
        "X = df.drop('stress_level', axis=1)\n",
        "y = df['stress_level']\n",
        "class_names = [f'Class {c}' for c in sorted(y.unique())]\n",
        "\n",
        "# 3. Split Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training set: {X_train.shape}, Test set: {X_test.shape}\\n\")"
      ],
      "metadata": {
        "id": "AuPilufbAxae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to evaluate models\n",
        "def evaluate_model(model, X_test, y_test, model_name):\n",
        "    print(f\"--- Evaluation for: {model_name} ---\")\n",
        "\n",
        "    # Get predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # 1. Accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"1. Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    # 2. Classification Report\n",
        "    print(\"\\n2. Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=class_names))\n",
        "\n",
        "    # 3. Confusion Matrix\n",
        "    print(\"\\n3. Confusion Matrix:\")\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
        "    disp.plot(cmap=plt.cm.Blues)\n",
        "    plt.title(f\"Confusion Matrix - {model_name}\")\n",
        "    plt.show() # Display the plot\n",
        "\n",
        "    print(\"--------------------------------------------------\\n\")\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "h2kSgIPEA-IK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To store results for final comparison\n",
        "model_accuracies = {}\n",
        "\n",
        "# --- Variation 1: KNN (k=3, uniform weights) ---\n",
        "# This was the 'best_k' you found in your other notebook\n",
        "model_1 = KNeighborsClassifier(n_neighbors=3)\n",
        "model_1.fit(X_train, y_train)\n",
        "acc1 = evaluate_model(model_1, X_test, y_test, \"Variation 1: KNN (k=3)\")\n",
        "model_accuracies[\"KNN (k=3)\"] = acc1\n",
        "\n",
        "# --- Variation 2: KNN (k=5, uniform weights) ---\n",
        "# This was the 'initial_knn' you used in your other notebook\n",
        "model_2 = KNeighborsClassifier(n_neighbors=5)\n",
        "model_2.fit(X_train, y_train)\n",
        "acc2 = evaluate_model(model_2, X_test, y_test, \"Variation 2: KNN (k=5)\")\n",
        "model_accuracies[\"KNN (k=5)\"] = acc2\n",
        "\n",
        "# --- Variation 3: KNN (k=3, distance weights) ---\n",
        "# This is a new variation that weights points by distance\n",
        "model_3 = KNeighborsClassifier(n_neighbors=3, weights='distance')\n",
        "model_3.fit(X_train, y_train)\n",
        "acc3 = evaluate_model(model_3, X_test, y_test, \"Variation 3: KNN (k=3, weights='distance')\")\n",
        "model_accuracies[\"KNN (k=3, distance)\"] = acc3"
      ],
      "metadata": {
        "id": "G7lxIxBdBADm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Final Comparison ---\n",
        "\n",
        "print(\"\\n--- Final Model Accuracy Comparison ---\")\n",
        "# Convert the dictionary to a pandas Series for easy printing\n",
        "accuracy_summary = pd.Series(model_accuracies).sort_values(ascending=False)\n",
        "print(accuracy_summary)"
      ],
      "metadata": {
        "id": "6qaU1oHiBB3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IT24100479_DecisionTree"
      ],
      "metadata": {
        "id": "Rd_Qd3_MBFa4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n",
        ")\n",
        "from sklearn.datasets import make_classification  # Not used, but for potential extensions\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "GoBnnDPTBTSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"preprocessed_stress_level_dataset.csv\")\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Class distribution:\\n{df['stress_level'].value_counts()}\")"
      ],
      "metadata": {
        "id": "K5oLKTOPBfMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Size of testing and training\n",
        "def prepare_data(df):\n",
        "    X = df.iloc[:, :-1].values  # Features: columns 0-11\n",
        "    y = df['stress_level'].values  # Target\n",
        "    return X, y\n",
        "\n",
        "def split_data(X, y, test_size=0.2, random_state=42):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
        "    )\n",
        "    print(f\"Training set size: {X_train.shape[0]}\")\n",
        "    print(f\"Test set size: {X_test.shape[0]}\")\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "X, y = prepare_data(df)\n",
        "X_train, X_test, y_train, y_test = split_data(X, y)"
      ],
      "metadata": {
        "id": "X_LfrW3uBjRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature names for plotting (assuming 12 features)\n",
        "feature_names = [f'feature_{i}' for i in range(X.shape[1])]\n",
        "class_names = ['0', '1', '2']  # Stress levels\n",
        "target_names = ['Low Stress', 'Medium Stress', 'High Stress']"
      ],
      "metadata": {
        "id": "Fj2h_clEBloF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Decision Tree Model\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Initialize and fit the model with random_state=42 using X_train and y_train\n",
        "model_dt = DecisionTreeClassifier(random_state=42)\n",
        "model_dt.fit(X_train, y_train)\n",
        "\n",
        "# Print max depth\n",
        "print(f\"Max Depth: {model_dt.get_depth()}\")\n",
        "\n",
        "# Print feature importances as a pandas Series\n",
        "importances = pd.Series(model_dt.feature_importances_, index=feature_names)\n",
        "print(\"Feature Importance:\")\n",
        "print(importances)"
      ],
      "metadata": {
        "id": "g41yw-HmBnxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate Model Performance\n",
        "# Predict on test set\n",
        "y_pred_dt = model_dt.predict(X_test)\n",
        "\n",
        "# Compute metrics (macro-averaged for multi-class)\n",
        "accuracy = accuracy_score(y_test, y_pred_dt)\n",
        "precision = precision_score(y_test, y_pred_dt, average='macro')\n",
        "recall = recall_score(y_test, y_pred_dt, average='macro')\n",
        "f1 = f1_score(y_test, y_pred_dt, average='macro')\n",
        "roc_auc = roc_auc_score(y_test, model_dt.predict_proba(X_test), multi_class='ovr', average='macro')\n",
        "\n",
        "print(f\"Decision Tree Metrics:\")\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-Score: {f1}\")\n",
        "print(f\"ROC AUC: {roc_auc}\")"
      ],
      "metadata": {
        "id": "dNs4qsRZBpP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the Decision Tree structure to visualize its hierarchical splits\n",
        "plt.figure(figsize=(12, 8))\n",
        "plot_tree(\n",
        "    model_dt,\n",
        "    feature_names=feature_names,\n",
        "    class_names=class_names,\n",
        "    filled=True\n",
        ")\n",
        "plt.title(\"Decision Tree Structure\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QbBSeG_mBq6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reduce the 12-feature dataset to 2D using PCA for decision boundary visualization\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)\n",
        "\n",
        "# Train a Decision Tree model on the 2D PCA-transformed training data to create a decision boundary\n",
        "model_dt_pca = DecisionTreeClassifier(random_state=42).fit(X_train_pca, y_train)\n",
        "\n",
        "# Create a mesh grid to predict class labels across the 2D PCA space for the decision boundary\n",
        "x_min, x_max = X_test_pca[:, 0].min() - 1, X_test_pca[:, 0].max() + 1\n",
        "y_min, y_max = X_test_pca[:, 1].min() - 1, X_test_pca[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))\n",
        "Z = model_dt_pca.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)"
      ],
      "metadata": {
        "id": "76VfJVUKBtir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the decision boundary and test data points to show class separation\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')\n",
        "scatter = plt.scatter(X_test_pca[:, 0], X_test_pca[:, 1], c=y_test, cmap='coolwarm', edgecolor='k')\n",
        "plt.xlabel(\"PCA Component 1\")\n",
        "plt.ylabel(\"PCA Component 2\")\n",
        "plt.title(\"Decision Tree Decision Boundary (PCA)\")\n",
        "plt.colorbar(scatter)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aUt7W9-gBviO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the confusion matrix to visualize classification performance\n",
        "cm = confusion_matrix(y_test, y_pred_dt)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names)\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ipoQmAfIBzcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment with Parameters and Analyze Behavior\n",
        "# Loop over max_depth values [1, 3, 10] for the Decision Tree model\n",
        "# For each max_depth, train a model, predict on X_test, and compute accuracy and F1-score\n",
        "\n",
        "depths = [1, 3, 10]\n",
        "results = []\n",
        "\n",
        "for depth in depths:\n",
        "    # Train model with max_depth=depth\n",
        "    model_dt_exp = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
        "    model_dt_exp.fit(X_train, y_train)\n",
        "    y_pred_exp = model_dt_exp.predict(X_test)\n",
        "\n",
        "    # Compute metrics\n",
        "    acc = accuracy_score(y_test, y_pred_exp)\n",
        "    precision = precision_score(y_test, y_pred_exp, average='macro')\n",
        "    recall = recall_score(y_test, y_pred_exp, average='macro')\n",
        "    f1 = f1_score(y_test, y_pred_exp, average='macro')\n",
        "    roc_auc = roc_auc_score(y_test, model_dt_exp.predict_proba(X_test), multi_class='ovr', average='macro')\n",
        "\n",
        "    print(f\"Decision Tree (max_depth={depth}) Metrics:\")\n",
        "    print(f\"Accuracy: {acc}\")\n",
        "    print(f\"Precision: {precision}\")\n",
        "    print(f\"Recall: {recall}\")\n",
        "    print(f\"F1-Score: {f1}\")\n",
        "    print(f\"ROC AUC: {roc_auc}\")\n",
        "\n",
        "    results.append({'Max Depth': depth, 'Accuracy': acc, 'Precision': precision, 'Recall': recall, 'F1-Score': f1, 'ROC AUC': roc_auc})"
      ],
      "metadata": {
        "id": "GVt1ARvJB1SQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print a summary table of results\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\nSummary Table:\")\n",
        "print(results_df)"
      ],
      "metadata": {
        "id": "nVQJaDc4B4-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IT24100821_SVM"
      ],
      "metadata": {
        "id": "hxgNekpuB7xf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, f1_score, precision_score, recall_score,\n",
        "    confusion_matrix, roc_auc_score\n",
        ")\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import uuid\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold"
      ],
      "metadata": {
        "id": "onI9EaDMCTZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('preprocessed_stress_level_dataset.csv')"
      ],
      "metadata": {
        "id": "PBZ4Y96SCZ-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = data.iloc[:, :-1]\n",
        "y = data['stress_level']\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "eY5IKrHfCbW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid_linear = {'C': [0.1, 1, 10]}\n",
        "param_grid_rbf_scale = {'C': [0.1, 1, 10]}\n",
        "param_grid_rbf_gamma = {'C': [0.1, 1, 10], 'gamma': [0.01, 0.1, 1]}"
      ],
      "metadata": {
        "id": "bi74S-0qCcyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize StratifiedKFold\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Linear SVM\n",
        "model_linear = SVC(kernel='linear', probability=True, random_state=42)\n",
        "grid_search_linear = GridSearchCV(model_linear, param_grid_linear, cv=cv, scoring='accuracy')\n",
        "grid_search_linear.fit(X_train, y_train)\n",
        "\n",
        "# RBF SVM with gamma='scale'\n",
        "model_rbf_scale = SVC(kernel='rbf', gamma='scale', probability=True, random_state=42)\n",
        "grid_search_rbf_scale = GridSearchCV(model_rbf_scale, param_grid_rbf_scale, cv=cv, scoring='accuracy')\n",
        "grid_search_rbf_scale.fit(X_train, y_train)\n",
        "\n",
        "# RBF SVM with specified gamma\n",
        "model_rbf_gamma = SVC(kernel='rbf', probability=True, random_state=42)\n",
        "grid_search_rbf_gamma = GridSearchCV(model_rbf_gamma, param_grid_rbf_gamma, cv=cv, scoring='accuracy')\n",
        "grid_search_rbf_gamma.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "2cPrd3SaCed1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the best models and their parameters\n",
        "best_linear_model = grid_search_linear.best_estimator_\n",
        "best_linear_params = grid_search_linear.best_params_\n",
        "print(f\"Best parameters for Linear SVM: {best_linear_params}\")\n",
        "\n",
        "best_rbf_scale_model = grid_search_rbf_scale.best_estimator_\n",
        "best_rbf_scale_params = grid_search_rbf_scale.best_params_\n",
        "print(f\"Best parameters for RBF SVM (gamma='scale'): {best_rbf_scale_params}\")\n",
        "\n",
        "best_rbf_gamma_model = grid_search_rbf_gamma.best_estimator_\n",
        "best_rbf_gamma_params = grid_search_rbf_gamma.best_params_\n",
        "print(f\"Best parameters for RBF SVM (specified gamma): {best_rbf_gamma_params}\")\n",
        "\n",
        "# Evaluate the best models on the test set\n",
        "best_models_results = []\n",
        "\n",
        "# Evaluate Linear SVM\n",
        "y_pred_linear = best_linear_model.predict(X_test)\n",
        "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
        "precision_linear = precision_score(y_test, y_pred_linear, average='weighted')\n",
        "recall_linear = recall_score(y_test, y_pred_linear, average='weighted')\n",
        "f1_linear = f1_score(y_test, y_pred_linear, average='weighted')\n",
        "\n",
        "lb = LabelBinarizer()\n",
        "y_test_bin = lb.fit_transform(y_test)\n",
        "y_pred_proba_linear = best_linear_model.predict_proba(X_test)\n",
        "auc_linear = roc_auc_score(y_test_bin, y_pred_proba_linear, multi_class='ovr')\n",
        "\n",
        "cm_linear = confusion_matrix(y_test, y_pred_linear)\n",
        "\n",
        "best_models_results.append({\n",
        "    'model': f\"Linear SVM (tuned: {best_linear_params})\",\n",
        "    'accuracy': accuracy_linear,\n",
        "    'precision': precision_linear,\n",
        "    'recall': recall_linear,\n",
        "    'f1': f1_linear,\n",
        "    'auc': auc_linear,\n",
        "    'confusion_matrix': cm_linear\n",
        "})\n",
        "\n",
        "# Evaluate RBF SVM with gamma='scale'\n",
        "y_pred_rbf_scale = best_rbf_scale_model.predict(X_test)\n",
        "accuracy_rbf_scale = accuracy_score(y_test, y_pred_rbf_scale)\n",
        "precision_rbf_scale = precision_score(y_test, y_pred_rbf_scale, average='weighted')\n",
        "recall_rbf_scale = recall_score(y_test, y_pred_rbf_scale, average='weighted')\n",
        "f1_rbf_scale = f1_score(y_test, y_pred_rbf_scale, average='weighted')\n",
        "\n",
        "y_pred_proba_rbf_scale = best_rbf_scale_model.predict_proba(X_test)\n",
        "auc_rbf_scale = roc_auc_score(y_test_bin, y_pred_proba_rbf_scale, multi_class='ovr')\n",
        "\n",
        "cm_rbf_scale = confusion_matrix(y_test, y_pred_rbf_scale)\n",
        "\n",
        "best_models_results.append({\n",
        "    'model': f\"RBF SVM (gamma='scale', tuned: {best_rbf_scale_params})\",\n",
        "    'accuracy': accuracy_rbf_scale,\n",
        "    'precision': precision_rbf_scale,\n",
        "    'recall': recall_rbf_scale,\n",
        "    'f1': f1_rbf_scale,\n",
        "    'auc': auc_rbf_scale,\n",
        "    'confusion_matrix': cm_rbf_scale\n",
        "})\n",
        "\n",
        "# Evaluate RBF SVM with specified gamma\n",
        "y_pred_rbf_gamma = best_rbf_gamma_model.predict(X_test)\n",
        "accuracy_rbf_gamma = accuracy_score(y_test, y_pred_rbf_gamma)\n",
        "precision_rbf_gamma = precision_score(y_test, y_pred_rbf_gamma, average='weighted')\n",
        "recall_rbf_gamma = recall_score(y_test, y_pred_rbf_gamma, average='weighted')\n",
        "f1_rbf_gamma = f1_score(y_test, y_pred_rbf_gamma, average='weighted')\n",
        "\n",
        "y_pred_proba_rbf_gamma = best_rbf_gamma_model.predict_proba(X_test)\n",
        "auc_rbf_gamma = roc_auc_score(y_test_bin, y_pred_proba_rbf_gamma, multi_class='ovr')\n",
        "\n",
        "cm_rbf_gamma = confusion_matrix(y_test, y_pred_rbf_gamma)\n",
        "\n",
        "best_models_results.append({\n",
        "    'model': f\"RBF SVM (specified gamma, tuned: {best_rbf_gamma_params})\",\n",
        "    'accuracy': accuracy_rbf_gamma,\n",
        "    'precision': precision_rbf_gamma,\n",
        "    'recall': recall_rbf_gamma,\n",
        "    'f1': f1_rbf_gamma,\n",
        "    'auc': auc_rbf_gamma,\n",
        "    'confusion_matrix': cm_rbf_gamma\n",
        "})\n",
        "\n",
        "# Print results\n",
        "print(\"\\nEvaluation of Best Tuned Models on Test Set:\")\n",
        "for result in best_models_results:\n",
        "    print(f\"\\nModel: {result['model']}\")\n",
        "    print(f\"Accuracy: {result['accuracy']:.4f}\")\n",
        "    print(f\"Precision: {result['precision']:.4f}\")\n",
        "    print(f\"Recall: {result['recall']:.4f}\")\n",
        "    print(f\"F1 Score: {result['f1']:.4f}\")\n",
        "    print(f\"AUC: {result['auc']:.4f}\")\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(result['confusion_matrix'])"
      ],
      "metadata": {
        "id": "ad0zJgiYCgYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect the evaluation results for the best models from each variation\n",
        "# best_models_results is already available from the previous step\n",
        "\n",
        "# Iterate through the results and print the evaluation metrics\n",
        "print(\"\\nComparison of Best Tuned Model Variations:\")\n",
        "for result in best_models_results:\n",
        "    print(f\"\\nModel: {result['model']}\")\n",
        "    print(f\"Accuracy: {result['accuracy']:.4f}\")\n",
        "    print(f\"Precision: {result['precision']:.4f}\")\n",
        "    print(f\"Recall: {result['recall']:.4f}\")\n",
        "    print(f\"F1 Score: {result['f1']:.4f}\")\n",
        "    print(f\"AUC: {result['auc']:.4f}\")\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(result['confusion_matrix'])\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    artifact_id = str(uuid.uuid4())\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(result['confusion_matrix'], annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title(f'Confusion Matrix: {result[\"model\"]}')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.savefig(f'confusion_matrix_tuned_{artifact_id}.png')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Determine and print the model with the highest accuracy\n",
        "best_overall_model = max(best_models_results, key=lambda x: x['accuracy'])\n",
        "\n",
        "print(f\"\\nOverall Best Model Based on Accuracy: {best_overall_model['model']}\")\n",
        "print(f\"Accuracy: {best_overall_model['accuracy']:.4f}\")"
      ],
      "metadata": {
        "id": "vVXMfgseChDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "IT24100890_Random_forest"
      ],
      "metadata": {
        "id": "GR27dfEGCk0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Import Required Libraries ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
        "import time\n",
        "\n",
        "# --- 2. Load and Split the Dataset ---\n",
        "\n",
        "# Load the preprocessed data\n",
        "df = pd.read_csv('preprocessed_stress_level_dataset.csv')\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = df.drop('stress_level', axis=1)\n",
        "y = df['stress_level']\n",
        "\n",
        "# Get feature names (as strings, since they are 0, 1, 2...)\n",
        "feature_names = X.columns.tolist()\n",
        "\n",
        "# Split the data: 80% training, 20% testing\n",
        "# We use stratify=y to ensure the class distribution is the same in train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"Original Dataset Shape: {df.shape}\")\n",
        "print(f\"Features: {X.shape[1]}, Target: {y.name}\")\n",
        "print(f\"Target classes: {np.unique(y)}\")\n",
        "print(f\"Training Set Shape: {X_train.shape}, Test Set Shape: {X_test.shape}\")\n",
        "\n",
        "# Dictionary to store test accuracies for final comparison\n",
        "model_accuracies = {}"
      ],
      "metadata": {
        "id": "rLs3t-6LCsks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. Model Variation 1: Baseline Random Forest (Default Parameters) ---\n",
        "print(\"--- Training Variation 1: Baseline Random Forest ---\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Initialize the model with random_state for reproducibility\n",
        "rf_base = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Train the model\n",
        "rf_base.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_base = rf_base.predict(X_test)\n",
        "y_prob_base = rf_base.predict_proba(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "acc_base = accuracy_score(y_test, y_pred_base)\n",
        "model_accuracies['Baseline (Defaults)'] = acc_base\n",
        "auc_base = roc_auc_score(y_test, y_prob_base, multi_class='ovr')\n",
        "\n",
        "print(f\"Baseline Model Training Time: {time.time() - start_time:.2f} seconds\")\n",
        "print(f\"Baseline Model Test Accuracy: {acc_base:.6f}\")\n",
        "print(f\"Baseline Model Test AUC Score (OvR): {auc_base:.6f}\")\n",
        "print(\"\\nClassification Report (Baseline):\")\n",
        "print(classification_report(y_test, y_pred_base))\n",
        "\n",
        "# Plot Confusion Matrix\n",
        "cm_base = confusion_matrix(y_test, y_pred_base)\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm_base, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=np.unique(y), yticklabels=np.unique(y))\n",
        "plt.title('Confusion Matrix - Baseline Random Forest')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n",
        "\n",
        "print(\"--------------------------------------------------\\n\")"
      ],
      "metadata": {
        "id": "oIq2rtMPCydG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4. Model Variation 2: Manually Tuned (Deeper Trees) ---\n",
        "print(\"--- Training Variation 2: Manually Tuned (n_estimators=200, max_depth=20) ---\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Initialize the model with more trees and a defined max depth\n",
        "rf_manual = RandomForestClassifier(n_estimators=200, max_depth=20, random_state=42, n_jobs=-1)\n",
        "\n",
        "# Train the model\n",
        "rf_manual.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_manual = rf_manual.predict(X_test)\n",
        "y_prob_manual = rf_manual.predict_proba(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "acc_manual = accuracy_score(y_test, y_pred_manual)\n",
        "model_accuracies['Manual (n=200, depth=20)'] = acc_manual\n",
        "auc_manual = roc_auc_score(y_test, y_prob_manual, multi_class='ovr')\n",
        "\n",
        "print(f\"Manual Tune Model Training Time: {time.time() - start_time:.2f} seconds\")\n",
        "print(f\"Manual Tune Model Test Accuracy: {acc_manual:.6f}\")\n",
        "print(f\"Manual Tune Model Test AUC Score (OvR): {auc_manual:.6f}\")\n",
        "print(\"\\nClassification Report (Manual Tune):\")\n",
        "print(classification_report(y_test, y_pred_manual))\n",
        "\n",
        "# Plot Confusion Matrix\n",
        "cm_manual = confusion_matrix(y_test, y_pred_manual)\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm_manual, annot=True, fmt='d', cmap='Greens',\n",
        "            xticklabels=np.unique(y), yticklabels=np.unique(y))\n",
        "plt.title('Confusion Matrix - Manual Tune Random Forest')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n",
        "\n",
        "print(\"--------------------------------------------------\\n\")"
      ],
      "metadata": {
        "id": "zyfd0zXuCzcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 5. Model Variation 3: GridSearchCV Tuned Model ---\n",
        "print(\"--- Training Variation 3: GridSearchCV Tuned ---\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Define the parameter grid to search\n",
        "# This is a small grid to run relatively quickly.\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 150],      # Number of trees\n",
        "    'max_depth': [10, 20, None],     # Max depth of trees\n",
        "    'min_samples_split': [2, 5],     # Min samples to split a node\n",
        "    'min_samples_leaf': [1, 2]       # Min samples at a leaf node\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "# cv=5 means 5-fold cross-validation\n",
        "# n_jobs=-1 uses all available CPU cores\n",
        "grid_search = GridSearchCV(estimator=RandomForestClassifier(random_state=42),\n",
        "                           param_grid=param_grid,\n",
        "                           cv=5,\n",
        "                           n_jobs=-1,\n",
        "                           scoring='accuracy',\n",
        "                           verbose=1)\n",
        "\n",
        "# Fit GridSearchCV to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(f\"\\nGridSearchCV Training Time: {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "# Get the best model\n",
        "best_rf = grid_search.best_estimator_\n",
        "print(f\"Best Parameters Found by GridSearchCV:\\n{grid_search.best_params_}\")\n",
        "\n",
        "# Make predictions with the best model\n",
        "y_pred_grid = best_rf.predict(X_test)\n",
        "y_prob_grid = best_rf.predict_proba(X_test)\n",
        "\n",
        "# Evaluate the best model\n",
        "acc_grid = accuracy_score(y_test, y_pred_grid)\n",
        "model_accuracies['GridSearchCV Tuned'] = acc_grid\n",
        "auc_grid = roc_auc_score(y_test, y_prob_grid, multi_class='ovr')\n",
        "\n",
        "print(f\"\\nGridSearchCV Tuned Model Test Accuracy: {acc_grid:.6f}\")\n",
        "print(f\"GridSearchCV Tuned Model Test AUC Score (OvR): {auc_grid:.6f}\")\n",
        "print(\"\\nClassification Report (GridSearchCV Tuned):\")\n",
        "print(classification_report(y_test, y_pred_grid))\n",
        "\n",
        "# Plot Confusion Matrix\n",
        "cm_grid = confusion_matrix(y_test, y_pred_grid)\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm_grid, annot=True, fmt='d', cmap='Oranges',\n",
        "            xticklabels=np.unique(y), yticklabels=np.unique(y))\n",
        "plt.title('Confusion Matrix - GridSearchCV Random Forest')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n",
        "\n",
        "print(\"--------------------------------------------------\\n\")"
      ],
      "metadata": {
        "id": "XaYiC9XYC1HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 6. Final Comparison ---\n",
        "\n",
        "# Print a summary of accuracies\n",
        "print(\"\\n--- Final Model Accuracy Comparison ---\")\n",
        "accuracy_series = pd.Series(model_accuracies).sort_values(ascending=False)\n",
        "print(accuracy_series)\n",
        "\n",
        "# Optional: Feature Importance from the best model\n",
        "print(f\"\\n--- Feature Importances (from {accuracy_series.index[0]} model) ---\")\n",
        "# Get the best model (assuming GridSearchCV was best, otherwise change 'best_rf')\n",
        "# You can change 'best_rf' to 'rf_base' or 'rf_manual' if one of them performed better\n",
        "best_model_for_features = grid_search.best_estimator_\n",
        "\n",
        "importances = best_model_for_features.feature_importances_\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(feature_importance_df.head(10))\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='Importance', y='Feature', data=feature_importance_df.head(10))\n",
        "plt.title('Top 10 Feature Importances')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "q8wnXeE2C21o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "IT24100307_MLP"
      ],
      "metadata": {
        "id": "aihpQZnODk-m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tensorflow scikit-learn pandas matplotlib seaborn\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, callbacks\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "\n",
        "df = pd.read_csv('preprocessed_stress_level_dataset.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "0sE8hVKWC4l2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate X and y\n",
        "X = df.drop(columns=['stress_level'])\n",
        "y = df['stress_level']\n",
        "\n",
        "# Train / test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Feature Scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"X_train shape:\", X_train_scaled.shape, \"X_test shape:\", X_test_scaled.shape)"
      ],
      "metadata": {
        "id": "kwM_jf-HDs2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a basic MLP model with one hidden layer, reduced neurons, and less epochs\n",
        "\n",
        "# Use the build_mlp function defined in the previous cell\n",
        "# Assuming input_dim and n_classes are already defined from data preprocessing\n",
        "input_dim = X_train_scaled.shape[1]\n",
        "n_classes = len(np.unique(y_train))\n",
        "\n",
        "model_reduced = build_mlp(input_dim=input_dim,\n",
        "                          n_classes=n_classes,\n",
        "                          hidden_layers=[16],\n",
        "                          dropout_rate=0.2,\n",
        "                          lr=1e-3)\n",
        "\n",
        "# Train the model with fewer epochs\n",
        "history_reduced = model_reduced.fit(\n",
        "    X_train_scaled, y_train,\n",
        "    validation_split=0.15,\n",
        "    epochs=20,\n",
        "    batch_size=32,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "model_reduced.summary()"
      ],
      "metadata": {
        "id": "Zqx549mbDvHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training history\n",
        "history_reduced_df = pd.DataFrame(history_reduced.history)\n",
        "history_reduced_df[['loss','val_loss']].plot(title='Loss (Reduced Model)', figsize=(8,4))\n",
        "plt.xlabel('Epoch')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "history_reduced_df[['accuracy','val_accuracy']].plot(title='Accuracy (Reduced Model)', figsize=(8,4))\n",
        "plt.xlabel('Epoch')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0ff-bVmcD2db"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the reduced model on the test set\n",
        "# Predictions\n",
        "if n_classes == 2:\n",
        "    y_proba = model_reduced.predict(X_test_scaled).ravel()\n",
        "    y_pred = (y_proba >= 0.5).astype(int)\n",
        "else:\n",
        "    y_proba = model_reduced.predict(X_test_scaled)\n",
        "    y_pred = np.argmax(y_proba, axis=1)\n",
        "\n",
        "print(\"Test accuracy (Reduced Model):\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report (Reduced Model):\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix (Reduced Model)')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "r6fxgX0ID5H6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Training\n",
        "input_dim = X_train_scaled.shape[1]\n",
        "n_classes = len(np.unique(y_train))\n",
        "\n",
        "model = build_mlp(input_dim=input_dim,\n",
        "                  n_classes=n_classes,\n",
        "                  hidden_layers=[128,64],\n",
        "                  dropout_rate=0.25, lr=1e-3)\n",
        "\n",
        "# Callbacks\n",
        "early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
        "\n",
        "history = model.fit(\n",
        "    X_train_scaled, y_train,\n",
        "    validation_split=0.15,\n",
        "    epochs=100,\n",
        "    batch_size=32,\n",
        "    callbacks=[early_stop, reduce_lr],\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "model_reduced.summary()"
      ],
      "metadata": {
        "id": "CExfOsKrD6ow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Plot training history\n",
        "history_df = pd.DataFrame(history.history)\n",
        "history_df[['loss','val_loss']].plot(title='Loss', figsize=(8,4))\n",
        "plt.xlabel('Epoch')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "history_df[['accuracy','val_accuracy']].plot(title='Accuracy', figsize=(8,4))\n",
        "plt.xlabel('Epoch')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "dx3Vs4bzD8H3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Evaluation\n",
        "# Predictions\n",
        "if n_classes == 2:\n",
        "    y_proba = model.predict(X_test_scaled).ravel()\n",
        "    y_pred = (y_proba >= 0.5).astype(int)\n",
        "else:\n",
        "    y_proba = model.predict(X_test_scaled)\n",
        "    y_pred = np.argmax(y_proba, axis=1)\n",
        "\n",
        "print(\"Test accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "nwey1VRlD9YP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Manual grid over a few hyperparameters (fast)\n",
        "results = []\n",
        "histories = {} # Dictionary to store training histories\n",
        "best_acc = 0 # Variable to store the best accuracy found\n",
        "best_model = None # Variable to store the best model\n",
        "\n",
        "hidden_options = [[64,32],[128,64],[256,128]]\n",
        "dropouts = [0.0, 0.2]\n",
        "for hidden in hidden_options:\n",
        "    for dp in dropouts:\n",
        "        key = f\"Hidden: {hidden}, Dropout: {dp}\"\n",
        "        print(f\"Training: {key}\")\n",
        "        m = build_mlp(input_dim=input_dim, n_classes=n_classes, hidden_layers=hidden, dropout_rate=dp, lr=1e-3)\n",
        "        h = m.fit(X_train_scaled, y_train, validation_split=0.12, epochs=40, batch_size=32, verbose=0,\n",
        "                  callbacks=[callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)])\n",
        "        histories[key] = h.history # Store the history dictionary\n",
        "\n",
        "        # evaluate quickly on test\n",
        "        if n_classes == 2:\n",
        "            yp = (m.predict(X_test_scaled).ravel() >= 0.5).astype(int)\n",
        "        else:\n",
        "            yp = np.argmax(m.predict(X_test_scaled), axis=1)\n",
        "        acc = accuracy_score(y_test, yp)\n",
        "        print(\"Acc:\", acc)\n",
        "        results.append({\"hidden\": hidden, \"dropout\": dp, \"acc\": acc})\n",
        "\n",
        "        # Check if this model is the best so far\n",
        "        if acc > best_acc:\n",
        "            best_acc = acc\n",
        "            best_model = m # Store the best model\n",
        "\n",
        "display(pd.DataFrame(results).sort_values('acc', ascending=False))\n",
        "model_reduced.summary()"
      ],
      "metadata": {
        "id": "d0dAMI5GD_or"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot accuracy and loss graphs for each hyperparameter combination from stored histories side-by-side\n",
        "if 'histories' not in locals() or not histories:\n",
        "    print(\"Error: Training histories not found. Please run the hyperparameter tuning cell first.\")\n",
        "else:\n",
        "    for key, history_data in histories.items():\n",
        "        history_df = pd.DataFrame(history_data)\n",
        "\n",
        "        print(f\"Plotting history for: {key}\")\n",
        "\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(10, 3)) # Create a figure with 2 subplots in one row\n",
        "\n",
        "        # Plot Loss on the first subplot\n",
        "        history_df[['loss', 'val_loss']].plot(ax=axes[0])\n",
        "        axes[0].set_title(f\"Loss - {key}\")\n",
        "        axes[0].set_xlabel('Epoch')\n",
        "        axes[0].grid(True)\n",
        "\n",
        "        # Plot Accuracy on the second subplot\n",
        "        history_df[['accuracy', 'val_accuracy']].plot(ax=axes[1])\n",
        "        axes[1].set_title(f\"Accuracy - {key}\")\n",
        "        axes[1].set_xlabel('Epoch')\n",
        "        axes[1].grid(True)\n",
        "\n",
        "        plt.tight_layout() # Adjust layout to prevent overlapping titles/labels\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "7wPfG3OiEBDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the best model on the test set and show confusion matrix / classification report\n",
        "\n",
        "# Assuming 'best_model' variable is available from the hyperparameter tuning step\n",
        "if 'best_model' not in locals():\n",
        "    print(\"Error: 'best_model' not found. Please run the hyperparameter tuning cell first.\")\n",
        "else:\n",
        "    print(\"Evaluating the best hyperparameter tuned model on the test set...\")\n",
        "\n",
        "    if n_classes == 2:\n",
        "        y_proba_best_eval = best_model.predict(X_test_scaled).ravel()\n",
        "        y_pred_best_eval = (y_proba_best_eval >= 0.5).astype(int)\n",
        "    else:\n",
        "        y_proba_best_eval = best_model.predict(X_test_scaled)\n",
        "        y_pred_best_eval = np.argmax(y_proba_best_eval, axis=1)\n",
        "\n",
        "    print(\"Test accuracy (Best Tuned Model):\", accuracy_score(y_test, y_pred_best_eval))\n",
        "    print(\"\\nClassification Report (Best Tuned Model):\\n\", classification_report(y_test, y_pred_best_eval))\n",
        "\n",
        "    # Confusion matrix for the best model\n",
        "    cm_best_eval = confusion_matrix(y_test, y_pred_best_eval)\n",
        "    plt.figure(figsize=(6,5))\n",
        "    sns.heatmap(cm_best_eval, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title('Confusion Matrix (Best Tuned Model)')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "eCM0pPM1ECvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare the base model, reduced model, and the best hyperparameter tuned model\n",
        "\n",
        "print(\"\\n--- variety 1 Performance ---\")\n",
        "# Evaluate the reduced model\n",
        "if 'model_reduced' not in locals():\n",
        "     print(\"Error: 'best_model' not found. Please run the hyperparameter tuning cell first.\")\n",
        "else:\n",
        "    if n_classes == 2:\n",
        "        y_proba_reduced_compare_zh = model_reduced.predict(X_test_scaled).ravel()\n",
        "        y_pred_reduced_compare_zh = (y_proba_reduced_compare_zh >= 0.5).astype(int)\n",
        "    else:\n",
        "        y_proba_reduced_compare_zh = model_reduced.predict(X_test_scaled)\n",
        "        y_pred_reduced_compare_zh = np.argmax(y_proba_reduced_compare_zh, axis=1)\n",
        "\n",
        "    acc_reduced_zh = accuracy_score(y_test, y_pred_reduced_compare_zh)\n",
        "    report_reduced_zh = classification_report(y_test, y_pred_reduced_compare_zh)\n",
        "\n",
        "    print(f\"Test Accuracy (variety 1): {acc_reduced_zh:.4f}\")\n",
        "    print(\"\\nClassification Report (variety 1):\\n\", report_reduced_zh)\n",
        "\n",
        "\n",
        "print(\"--- variety 2 ---\")\n",
        "# Re-evaluate the base model to ensure comparison is based on the same test set split\n",
        "# Assuming 'model' variable holds the base model from the initial training with callbacks\n",
        "if 'model' not in locals():\n",
        "    print(\"Error: 'best_model' not found. Please run the hyperparameter tuning cell first.\")\n",
        "else:\n",
        "    if n_classes == 2:\n",
        "        y_proba_base_compare_zh = model.predict(X_test_scaled).ravel()\n",
        "        y_pred_base_compare_zh = (y_proba_base_compare_zh >= 0.5).astype(int)\n",
        "    else:\n",
        "        y_proba_base_compare_zh = model.predict(X_test_scaled)\n",
        "        y_pred_base_compare_zh = np.argmax(y_proba_base_compare_zh, axis=1)\n",
        "\n",
        "    acc_base_zh = accuracy_score(y_test, y_pred_base_compare_zh)\n",
        "    report_base_zh = classification_report(y_test, y_pred_base_compare_zh)\n",
        "\n",
        "    print(f\"Test Accuracy (variety 2 - with Callbacks): {acc_base_zh:.4f}\")\n",
        "    print(\"\\nClassification Report (variety 2 - with Callbacks):\\n\", report_base_zh)\n",
        "\n",
        "\n",
        "print(\"\\n--- variety 3 ---\")\n",
        "# Re-evaluate the best tuned model to ensure comparison is based on the same test set split\n",
        "# Assuming 'best_model' variable holds the best tuned model\n",
        "if 'best_model' not in locals():\n",
        "     print(\"Error: 'best_model' not found. Please run the hyperparameter tuning cell first.\")\n",
        "else:\n",
        "    if n_classes == 2:\n",
        "        y_proba_best_compare_zh = best_model.predict(X_test_scaled).ravel()\n",
        "        y_pred_best_compare_zh = (y_proba_best_compare_zh >= 0.5).astype(int)\n",
        "    else:\n",
        "        y_proba_best_compare_zh = best_model.predict(X_test_scaled)\n",
        "        y_pred_best_compare_zh = np.argmax(y_proba_best_compare_zh, axis=1)\n",
        "\n",
        "    acc_best_zh = accuracy_score(y_test, y_pred_best_compare_zh)\n",
        "    report_best_zh = classification_report(y_test, y_pred_best_compare_zh)\n",
        "\n",
        "    print(f\"Test Accuracy (variety 3): {acc_best_zh:.4f}\")\n",
        "    print(\"\\nClassification Report (variety 3):\\n\", report_best_zh)\n",
        "\n",
        "# Quick comparison summary\n",
        "print(\"\\n--- Summary Comparison ---\")\n",
        "# Check if variables exist before printing summary\n",
        "if 'acc_reduced_zh' in locals():\n",
        "    print(f\"variety 1: {acc_reduced_zh:.4f}\")\n",
        "if 'acc_base_zh' in locals():\n",
        "    print(f\"variety 2: {acc_base_zh:.4f}\")\n",
        "if 'acc_best_zh' in locals():\n",
        "    print(f\"variety 3: {acc_best_zh:.4f}\")\n",
        "\n",
        "# Add comparison logic based on which models were evaluated\n",
        "accuracies = {}\n",
        "if 'acc_base_zh' in locals():\n",
        "    accuracies['Base Model'] = acc_base_zh\n",
        "if 'acc_reduced_zh' in locals():\n",
        "    accuracies['Reduced Model'] = acc_reduced_zh\n",
        "if 'acc_best_zh' in locals():\n",
        "    accuracies['Best Tuned Model'] = acc_best_zh\n",
        "\n",
        "if accuracies:\n",
        "    best_model_name = max(accuracies, key=accuracies.get)\n",
        "    print(f\"\\nBased on test accuracy, the '{best_model_name}' performed best.\")\n",
        "else:\n",
        "    print(\"\\nNo models were evaluated for comparison.\")"
      ],
      "metadata": {
        "id": "d2h6nM-kEENS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}